
# Stages:
#   0) Train VAE on full fields G_f (reconstruction). 
#   1) Train sparse recon model G_d -> G_f. 
#   2) Freeze VAE. Encode full fields to latent traj, roll out with temporal model for T_fore > 1, decode to fields.
#   x) Downstream: Given sparse obs window (N_window) and T_fore, produce G_f forecast [B, T_fore, N_pt, C].

# Notes ________________________________________
# ...
#_______________________________________________

case_index        : 0      # used for backup name
Stage             : 1       # 0: Only sparse reconstruction 1: Only temporal propagation -1: Train both together
Repeat_id         : 0       # Different propagator id sharing the same encoder and decoder

# Data______________________________________________
data_h5           : 2D_CollinearPlateFlow/Dataset/processed/Data2PlatesGap1Re40.h5

num_workers       : 8
train_ratio       : 0.90
batch_size        : 128
num_samples       : 1024         # Resample of cases in one epoch

channel           : 0               # Choose between 0, 1,...-1. For channel =-1 the code load all fields
process_mode      : MeanStdStand    # Choose between MinMaxNorm, MeanStdStand, SymLogQuant and None

global_restriction     : False         # Determine if we restrict the whole reconstruction area
sample_restriction     : False         # Determine if we restrict the sampling area
Sample_Parameters:                    # Regions that we limit sampling between (-1, 1)
  x_lo              : -1.0
  x_hi              : 1.0
  y_lo              : -1.0
  y_hi              : 1.0

Full_Field_DownS  : 1.0        # No Downsampling for CNN model
Num_x             : 88
Num_y             : 300
delta_t           : 0.01

num_time_sample   : 1          # N_ts, for encoder to generate latent trajectory
num_space_sample  : 256         # N_xs, spatial points randomly sampled
multi_factor      : 1.0
N_window          : 1

# Model______________________________________________

# For the Beta-VAE part
latent_dim      : 64
enc_channels    : [8, 16, 32, 64, 128, 256]   # growing
dec_channels    : [256, 128, 64, 32, 16, 8]   # mirror of enc_channels
kernel_size     : 3
stride          : 2
padding         : 1
activation      : "elu"      # relu / gelu / silu …
linear_hidden   : 256

# For the MLP-CNN-reconstructor, Input dim = N_obs × (2 + C)
embedding_size    : [75, 11]  # (H_emb, W_emb), Num_y / H_emb and Num_x / W_emb are close to powers of 2
hidden_dims       : [256, 256]
conv_channels     : [32, 64, 96, 96]

# Training______________________________________________
num_epochs        : 50000

learning_rate     : 3.0e-4
weight_decay      : 1.0e-6       # Weight decay for AdamW
warmup_epochs     : 50
monitor_every     : 5
patience_epochs   : 1000

beta_final        : 0.005
beta_init         : 0.001
beta_warmup       : 200

Loss_Traj_Weight  : 1.0

# folders -------------------------------------------------
save_loss_dir     : Save_loss_files/collinear_flow_Re40/train_test_loss_VAE
save_net_dir      : Output_Net/collinear_flow_Re40/Net_VAE
save_recon_dir    : Save_reconstruction_files/collinear_flow_Re40/Results_VAE
