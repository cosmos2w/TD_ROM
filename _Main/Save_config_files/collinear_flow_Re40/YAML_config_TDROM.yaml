
# Notes ________________________________________
# Repeat 7; in Original reconstructor, utilized new fusion order:

# fused_concat = torch.cat([local_lat, cls_proj_bt], dim=-1)  # (B*T, P, 2*d_model)
# fused_lat    = self.fusion_proj(fused_concat)
# fused_pre    = coord_tok + fused_lat

#_______________________________________________

device_ids        : [1]

case_index        : 12
Stage             : 0
# 0: Only sparse reconstruction 
# 1: Only temporal propagation 
# -1: Train both together
Repeat_id         : 0 

# data ----------------------------------------------------
data_h5           : 2D_CollinearPlateFlow/Dataset/processed/Data2PlatesGap1Re40.h5

Spatial_Dim       : 2          # Spatial dimension: 1, 2 or 3
delta_t           : 0.001      
U_dim             : 1          # Dummy in this case

N_window          : 1
num_time_sample   : 1          # N_ts, for encoder to generate latent trajectory
num_space_sample  : 64         # N_xs, spatial points randomly sampled
multi_factor      : 1.0

num_workers       : 8
train_ratio       : 0.90
batch_size        : 256
num_samples       : 1024            # Resample of cases in one epoch
Full_Field_DownS  : 0.30            # Downsampling ratio for reconstruction evaluation in training

channel           : 0               # Choose between 0, 1,...-1. For channel =-1 the code load all fields
process_mode      : MeanStdStand    # Choose between MinMaxNorm, MeanStdStand, SymLogQuant and None

Use_Adaptive_Selection : True          # Enable Bayesian φ
domain_decompose       : True          # Enable adaptive domain decomposition

global_restriction     : False         # Determine if we restrict the whole reconstruction area
sample_restriction     : False         # Determine if we restrict the sampling area
Sample_Parameters:                     # Regions that we limit sampling
  x_lo              : -0.6
  x_hi              : 0.6
  y_lo              : -0.9
  y_hi              : 0.6

bandwidth_init : 0.05
top_k : 32
per_sensor_sigma : True  
overlap_ratio: 0.02
importance_scale: 0.25

# model ---------------------------------------------------
Reload_Trained    : False

F_dim             : 64
num_heads         : 16
num_layers        : 4
num_layers_propagator : 4
latent_tokens     : 64               # For cross-att in perceiver reconstructor

decoder_type      : CausalTrans         # LinTrans or DelayNODE or *CausalTrans or StdTrans or UD_Trans
reconstructor_type: Perceiver        # Perceiver or DeepONet

num_freqs         : 256
pooling           : "none"             # "mean" or "cls" or "none"
hidden_dims       : [512, 512, 512]    # for the DelayNODE

# training -----------------------------------------------
num_epochs        : 100000

learning_rate     : 3.0e-4
weight_decay      : 1.0e-6       # Weight decay for AdamW
warmup_epochs     : 50
monitor_every     : 5
patience_epochs   : 1000

CalRecVar         : True
retain_cls        : True
Supervise_Sensors : True
use_temporal      : False

Loss_Traj_Weight  : 1.0

nll_anneal_epochs : 500
nll_weight        : 0.00001

psd_weight        : 0.10
spectrum_weight   : 0.10
spectral_blend_weight : 0.5

# sparse adaptation -----------------------------------------
bayesian_phi:
  update_in_stage1 : True           # allow updates of mlp_phi in Stage-1 (temporal roll-out)
  phi_mlp_hidden_dim: 128           # Hidden size for φ MLP
  prior_alpha: 1.0                  # Beta prior: alpha (e.g., 2.0 biases toward retaining points early)
  prior_beta: 1.0                   # Beta prior: beta (e.g., 5.0 for lower drop-off probs initially)
  mc_samples_elbo: 5
  ema_alpha: 0.25                   # EMA decay for aggregating residuals (0.1 = slow adaptation)

  vi_entropy_weight: 0.020
  var_weight       : 0.20
  lambda_kl: 0.02                     # Weight for the kl loss to pull towards prior
  lambda_elbo: 0.001                 # Weight for the (negative) ELBO term in the main loss (integrates VI into single objective)

  anneal_epochs: 100                # Epochs to anneal threshold from low (retain ~1024) to high
  min_retain: 128                    # Minimum points to retain (fallback if too aggressive)
  max_retain: 128                    # Maximum points early on (warm-up)

  viz_every: 1000                    # Visualize φ every N epochs
  viz_save_dir: Save_report_files/collinear_flow_Re40           # Directory for φ PNG saves

# folders -------------------------------------------------
save_loss_dir     : Save_loss_files/collinear_flow_Re40/train_test_loss_TD-ROM
save_net_dir      : Output_Net/collinear_flow_Re40/Net_TD-ROM
save_recon_dir    : Save_reconstruction_files/collinear_flow_Re40/Results_TDROM